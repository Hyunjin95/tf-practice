{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2_tutorials_basic_with_keras",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyunjin95/tf-practice/blob/master/tf2_tutorials_basic_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82RPCBpWbi3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For now, you need to install the latest version of TensorFlow whenever you open a new runtime session.\n",
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zU3mLAbE-YI",
        "colab_type": "text"
      },
      "source": [
        "# Hello World for TensorFlow2\n",
        "\n",
        "https://www.tensorflow.org/tutorials/quickstart/beginner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK8iW8OoDPiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "model.evaluate(x_test, y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaAklO8IFQje",
        "colab_type": "text"
      },
      "source": [
        "# Fashion-MNIST with TensorFlow2.0 and Keras\n",
        "\n",
        "\n",
        "https://www.tensorflow.org/tutorials/keras/classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCOt2J0XFndI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# plt.figure(figsize=(10,10))\n",
        "# for i in range(25):\n",
        "#     plt.subplot(5, 5, i+1)\n",
        "#     plt.xticks([])\n",
        "#     plt.yticks([])\n",
        "#     plt.grid(False)\n",
        "#     plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "#     plt.xlabel(class_names[train_labels[i]])\n",
        "# plt.show()\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "predictions = model.predict(test_images)\n",
        "\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    if predicted_label == true_label:\n",
        "        color = 'blue'\n",
        "    else:\n",
        "        color = 'red'\n",
        "\n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(\n",
        "        class_names[predicted_label],\n",
        "        100*np.max(predictions_array),\n",
        "        class_names[true_label]),\n",
        "        color=color\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "    predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    this_plot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "    plt.ylim([0, 1])\n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "    this_plot[predicted_label].set_color('red')\n",
        "    this_plot[true_label].set_color('blue')\n",
        "\n",
        "\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "    plot_image(i, predictions, test_labels, test_images)\n",
        "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "    plot_value_array(i, predictions, test_labels)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "img = test_images[0]\n",
        "img = (np.expand_dims(img, 0))\n",
        "print(img.shape)\n",
        "predictions_single = model.predict(img)\n",
        "print(predictions_single)\n",
        "plot_value_array(0, predictions_single, test_labels)\n",
        "_ = plt.xticks(range(10), class_names, rotation=45)\n",
        "print(np.argmax(predictions_single[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxRPrAcjGJSL",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification with TF Hub and Keras\n",
        "\n",
        "https://www.tensorflow.org/tutorials/keras/text_classification_with_hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QnBtNFHGPyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# print(\"Version: \", tf.__version__)\n",
        "# print(\"Eager mode: \", tf.executing_eagerly())\n",
        "# print(\"Hub version: \", hub.__version__)\n",
        "# print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n",
        "\n",
        "# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n",
        "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
        "train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])\n",
        "\n",
        "(train_data, validation_data), test_data = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    split=(train_validation_split, tfds.Split.TEST),\n",
        "    as_supervised=True\n",
        ")\n",
        "\n",
        "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
        "# train_examples_batch\n",
        "# train_labels_batch\n",
        "\n",
        "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "hub_layer = hub.KerasLayer(\n",
        "    embedding,\n",
        "    input_shape=[], \n",
        "    dtype=tf.string,\n",
        "    trainable=True\n",
        ")\n",
        "hub_layer(train_examples_batch[:3])\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    hub_layer,\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_data.shuffle(10000).batch(512),\n",
        "    epochs=20,\n",
        "    validation_data=validation_data.batch(512),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "results = model.evaluate(test_data.batch(512), verbose=2)\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "    print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBmVBujeNwsK",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification with Preprocessed Text\n",
        "\n",
        "https://www.tensorflow.org/tutorials/keras/text_classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIDv5KXNN5_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "(train_data, test_data), info = tfds.load(\n",
        "    # Use the version pre-encoded with an ~8k vocabulary.\n",
        "    'imdb_reviews/subwords8k', \n",
        "    # Return the train/test datasets as a tuple.\n",
        "    split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "    # Return (example, label) pairs from the dataset (instead of a dictionary).\n",
        "    as_supervised=True,\n",
        "    # Also return the `info` structure. \n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "encoder = info.features['text'].encoder\n",
        "# print('Vocabulary size: {}'.format(encoder.vocab_size))\n",
        "\n",
        "\n",
        "# Try the encoder\n",
        "# sample_string = 'Hello, World!'\n",
        "# encoded_string = encoder.encode(sample_string)\n",
        "# print('Encoded string is {}'.format(encoded_string))\n",
        "# decoded_string = encoder.decode(encoded_string)\n",
        "# print('Decoded string is {}'.format(decoded_string))\n",
        "# assert sample_string == decoded_string\n",
        "\n",
        "# for ts in encoded_string:\n",
        "#     print ('{} ----> {}'.format(ts, encoder.decode([ts])))\n",
        "\n",
        "\n",
        "# Explore the data\n",
        "# for train_example, train_label in train_data.take(1):\n",
        "#     print('Encoded text:', train_example[:10].numpy())\n",
        "#     print('Label:', train_label.numpy())\n",
        "# encoder.decode(train_example)\n",
        "\n",
        "\n",
        "# Prepare the data for training\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = (\n",
        "    train_data\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .padded_batch(32, tf.compat.v1.data.get_output_shapes(train_data))\n",
        ")\n",
        "\n",
        "test_batches = (\n",
        "    test_data\n",
        "        .padded_batch(32, tf.compat.v1.data.get_output_shapes(train_data))\n",
        ")\n",
        "\n",
        "# for example_batch, label_batch in train_batches.take(3):\n",
        "#     print(\"Batch shape:\", example_batch.shape)\n",
        "#     print(\"Label shape:\", label_batch.shape)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(encoder.vocab_size, 16),\n",
        "    keras.layers.GlobalAveragePooling1D(),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_batches,\n",
        "    epochs=10,\n",
        "    validation_data=test_batches,\n",
        "    validation_steps=30\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_batches)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "# Create a graph\n",
        "history_dict = history.history\n",
        "history_dict.keys()\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc)+1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label=\"Validation loss\")\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3EpKrBcaduj",
        "colab_type": "text"
      },
      "source": [
        "# Basic regression: Predict fuel efficiency\n",
        "\n",
        "https://www.tensorflow.org/tutorials/keras/regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSxkaAJDaft6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use seaborn for pairplot\n",
        "!pip install -q seaborn\n",
        "# Use some functions from tensorflow_docs\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling\n",
        "\n",
        "# Get the data\n",
        "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
        "# dataset_path\n",
        "\n",
        "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
        "                            'Acceleration', 'Model Year', 'Origin']\n",
        "raw_dataset = pd.read_csv(\n",
        "    dataset_path,\n",
        "    names=column_names,\n",
        "    na_values = \"?\",\n",
        "    comment='\\t',\n",
        "    sep=\" \",\n",
        "    skipinitialspace=True\n",
        ")\n",
        "\n",
        "dataset = raw_dataset.copy()\n",
        "dataset.tail()\n",
        "\n",
        "# Clean the data\n",
        "dataset.isna().sum()\n",
        "dataset = dataset.dropna()\n",
        "# Convert the categorical column into a one-hot\n",
        "dataset['Origin'] = dataset['Origin'].map(lambda x: {1: 'USA', 2: 'Europe', 3: 'Japan'}.get(x))\n",
        "\n",
        "dataset = pd.get_dummies(dataset, prefix=' ', prefix_sep=' ')\n",
        "dataset.tail()\n",
        "\n",
        "\n",
        "# Split the data into train and test\n",
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "# Inspect the data\n",
        "sns.pairplot(train_dataset[[\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\"]], diag_kind=\"kde\")\n",
        "\n",
        "train_stats = train_dataset.describe()\n",
        "train_stats.pop(\"MPG\")\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats\n",
        "\n",
        "\n",
        "# Split features from labels\n",
        "train_labels = train_dataset.pop('MPG')\n",
        "test_labels = test_dataset.pop('MPG')\n",
        "\n",
        "\n",
        "# Normailze the data\n",
        "def norm(x):\n",
        "    return (x - train_stats['mean']) / train_stats['std']\n",
        "\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)\n",
        "\n",
        "\n",
        "# Build the model\n",
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "\n",
        "    model.compile(\n",
        "        loss = 'mse',\n",
        "        optimizer=optimizer,\n",
        "        metrics=['mae', 'mse']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "\n",
        "# Inspect the model\n",
        "# model.summary()\n",
        "\n",
        "# example_batch = normed_train_data[:10]\n",
        "# example_result = model.predict(example_batch)\n",
        "# example_result\n",
        "\n",
        "\n",
        "# Train the model\n",
        "EPOCHS = 1000\n",
        "\n",
        "history = model.fit(\n",
        "    normed_train_data,\n",
        "    train_labels,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split = 0.2,\n",
        "    verbose=0,\n",
        "    callbacks=[tfdocs.modeling.EpochDots()]\n",
        ")\n",
        "\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "# hist.tail()\n",
        "\n",
        "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
        "plotter.plot({'Basic': history}, metric = \"mae\")\n",
        "plt.ylim([0, 10])\n",
        "plt.ylabel('MAE [MPG]')\n",
        "\n",
        "plotter.plot({'Basic': history}, metric = \"mse\")\n",
        "plt.ylim([0, 20])\n",
        "plt.ylabel('MSE [MPG^2]')\n",
        "\n",
        "\n",
        "# Stop the model when the result doesn't improve\n",
        "model = build_model()\n",
        "\n",
        "# The patience parameter is the amount of epochs to check for improvement\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "early_history = model.fit(\n",
        "                            normed_train_data,\n",
        "                            train_labels, \n",
        "                            epochs=EPOCHS,\n",
        "                            validation_split = 0.2,\n",
        "                            verbose=0,\n",
        "                            callbacks=[early_stop, tfdocs.modeling.EpochDots()]\n",
        "                        )\n",
        "\n",
        "plotter.plot({'Early Stopping': early_history}, metric = \"mae\")\n",
        "plt.ylim([0, 10])\n",
        "plt.ylabel('MAE [MPG]')\n",
        "\n",
        "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "test_predictions = model.predict(normed_test_data).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "lims = [0, 50]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)\n",
        "\n",
        "\n",
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins = 25)\n",
        "plt.xlabel(\"Prediction Error [MPG]\")\n",
        "_ = plt.ylabel(\"Count\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1eZZrT_gpFN",
        "colab_type": "text"
      },
      "source": [
        "# Overfit and underfit\n",
        "\n",
        "https://www.tensorflow.org/tutorials/keras/overfit_and_underfit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbm3XTn4gr6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use some functions from tensorflow_docs\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling\n",
        "\n",
        "from  IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pathlib\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
        "shutil.rmtree(logdir, ignore_errors=True)\n",
        "\n",
        "\n",
        "# Get the dataset\n",
        "gz = tf.keras.utils.get_file('HIGGS.csv.gz', 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')\n",
        "\n",
        "FEATURES = 28\n",
        "ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")\n",
        "\n",
        "def pack_row(*row):\n",
        "    label = row[0]\n",
        "    features = tf.stack(row[1:],1)\n",
        "    return features, label\n",
        "\n",
        "packed_ds = ds.batch(10000).map(pack_row).unbatch()\n",
        "\n",
        "for features,label in packed_ds.batch(1000).take(1):\n",
        "    print(features[0])\n",
        "    plt.hist(features.numpy().flatten(), bins = 101)\n",
        "\n",
        "N_VALIDATION = int(1e3)\n",
        "N_TRAIN = int(1e4)\n",
        "BUFFER_SIZE = int(1e4)\n",
        "BATCH_SIZE = 500\n",
        "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n",
        "\n",
        "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
        "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()\n",
        "\n",
        "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "# Demonstrate overfitting\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    0.001,\n",
        "    decay_steps=STEPS_PER_EPOCH*1000,\n",
        "    decay_rate=1,\n",
        "    staircase=False\n",
        ")\n",
        "\n",
        "def get_optimizer():\n",
        "    return tf.keras.optimizers.Adam(lr_schedule)\n",
        "\n",
        "step = np.linspace(0, 100000)\n",
        "lr = lr_schedule(step)\n",
        "plt.figure(figsize = (8,6))\n",
        "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "_ = plt.ylabel('Learning Rate')\n",
        "\n",
        "def get_callbacks(name):\n",
        "    return [\n",
        "        tfdocs.modeling.EpochDots(),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
        "        tf.keras.callbacks.TensorBoard(logdir/name)\n",
        "    ]\n",
        "\n",
        "def compile_and_fit(model, name, optimizer=None, max_epochs=10000):\n",
        "    if optimizer is None:\n",
        "        optimizer = get_optimizer()\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', 'binary_crossentropy']\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        steps_per_epoch=STEPS_PER_EPOCH,\n",
        "        epochs=max_epochs,\n",
        "        validation_data=validate_ds,\n",
        "        callbacks=get_callbacks(name),\n",
        "        verbose=0\n",
        "    )\n",
        "    return history\n",
        "\n",
        "\n",
        "# Tiny model\n",
        "tiny_model = tf.keras.Sequential([\n",
        "    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "size_histories = {}\n",
        "size_histories['Tiny'] = compile_and_fit(tiny_model, 'sizes/Tiny')\n",
        "\n",
        "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
        "plotter.plot(size_histories)\n",
        "plt.ylim([0.5, 0.7])\n",
        "\n",
        "\n",
        "# Small model\n",
        "small_model = tf.keras.Sequential([\n",
        "    layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(16, activation='elu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "size_histories['Small'] = compile_and_fit(small_model, 'sizes/Small')\n",
        "\n",
        "\n",
        "# Medium model\n",
        "medium_model = tf.keras.Sequential([\n",
        "    layers.Dense(64, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(64, activation='elu'),\n",
        "    layers.Dense(64, activation='elu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "size_histories['Medium']  = compile_and_fit(medium_model, \"sizes/Medium\")\n",
        "\n",
        "\n",
        "# Large model\n",
        "large_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "size_histories['large'] = compile_and_fit(large_model, \"sizes/large\")\n",
        "\n",
        "\n",
        "# Plot the training and validation losses\n",
        "plotter.plot(size_histories)\n",
        "a = plt.xscale('log')\n",
        "plt.xlim([5, max(plt.xlim())])\n",
        "plt.ylim([0.5, 0.7])\n",
        "plt.xlabel(\"Epochs [Log Scale]\")\n",
        "\n",
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97\",\n",
        "    width=\"100%\", height=\"800px\"\n",
        ")\n",
        "\n",
        "\n",
        "# Prevent overfitting - L2 Regularization\n",
        "shutil.rmtree(logdir/'regularizers/Tiny', ignore_errors=True)\n",
        "shutil.copytree(logdir/'sizes/Tiny', logdir/'regularizers/Tiny')\n",
        "\n",
        "regularizer_histories = {}\n",
        "regularizer_histories['Tiny'] = size_histories['Tiny']\n",
        "\n",
        "l2_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001),\n",
        "                 input_shape=(FEATURES,)),\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dense(512, activation='elu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "regularizer_histories['l2'] = compile_and_fit(l2_model, \"regularizers/l2\")\n",
        "plotter.plot(regularizer_histories)\n",
        "plt.ylim([0.5, 0.7])\n",
        "\n",
        "\n",
        "# Dropout\n",
        "dropout_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "regularizer_histories['dropout'] = compile_and_fit(dropout_model, \"regularizers/dropout\")\n",
        "plotter.plot(regularizer_histories)\n",
        "plt.ylim([0.5, 0.7])\n",
        "\n",
        "\n",
        "# Combine L2 + Dropout\n",
        "combined_model = tf.keras.Sequential([\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu', input_shape=(FEATURES,)),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
        "                 activation='elu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "regularizer_histories['combined'] = compile_and_fit(combined_model, \"regularizers/combined\")\n",
        "plotter.plot(regularizer_histories)\n",
        "plt.ylim([0.5, 0.7])\n",
        "\n",
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/fGInKDo8TXes1z7HQku9mw/#scalars&_smoothingWeight=0.97\",\n",
        "    width = \"100%\",\n",
        "    height=\"800px\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugTh1-8XVNMt",
        "colab_type": "text"
      },
      "source": [
        "# Save and Load Models\n",
        "\n",
        "https://www.tensorflow.org/tutorials/keras/save_and_load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmwPm_QxVRnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup\n",
        "!pip install -q pyyaml h5py\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.version.VERSION)\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_labels = train_labels[:1000]\n",
        "test_labels = test_labels[:1000]\n",
        "\n",
        "train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
        "test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0\n",
        "\n",
        "\n",
        "# Define a simple sequential model\n",
        "def create_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
        "        keras.layers.Dropout(0.2),\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Create a basic model instance\n",
        "model = create_model()\n",
        "\n",
        "# Display the model's architecture\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Save checkpoints during training\n",
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model with the new callback\n",
        "model.fit(\n",
        "    train_images, \n",
        "    train_labels,  \n",
        "    epochs=10,\n",
        "    validation_data=(test_images,test_labels),\n",
        "    callbacks=[cp_callback] # Pass callback to training\n",
        ")  \n",
        "\n",
        "# This may generate warnings related to saving the state of the optimizer.\n",
        "# These warnings (and similar warnings throughout this notebook)\n",
        "# are in place to discourage outdated usage, and can be ignored.\n",
        "\n",
        "!ls {checkpoint_dir}\n",
        "\n",
        "\n",
        "# Create a basic model instance\n",
        "model = create_model()\n",
        "\n",
        "# Evaluate the model\n",
        "loss, acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(\"Untrained model, accuracy: {:5.2f}%\".format(100*acc))\n",
        "\n",
        "\n",
        "# Loads the weights\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "# Re-evaluate the model\n",
        "loss,acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
        "\n",
        "\n",
        "# Checkpoint callback options\n",
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    period=5\n",
        ")\n",
        "\n",
        "# Create a new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Save the weights using the `checkpoint_path` format\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "# Train the model with the new callback\n",
        "model.fit(\n",
        "    train_images, \n",
        "    train_labels,\n",
        "    epochs=50, \n",
        "    callbacks=[cp_callback],\n",
        "    validation_data=(test_images, test_labels),\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "!ls {checkpoint_dir}\n",
        "\n",
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "# Create a new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Load the previously saved weights\n",
        "model.load_weights(latest)\n",
        "\n",
        "# Re-evaluate the model\n",
        "loss, acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
        "\n",
        "\n",
        "# Manually save weights\n",
        "# Save the weights\n",
        "model.save_weights('./checkpoints/my_checkpoint')\n",
        "\n",
        "# Create a new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Restore the weights\n",
        "model.load_weights('./checkpoints/my_checkpoint')\n",
        "\n",
        "# Evaluate the model\n",
        "loss,acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
        "\n",
        "\n",
        "\n",
        "# Save the entire model\n",
        "# Create and train a new model instance.\n",
        "model = create_model()\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Save the entire model to a HDF5 file.\n",
        "# The '.h5' extension indicates that the model shuold be saved to HDF5.\n",
        "model.save('my_model.h5') \n",
        "\n",
        "# Recreate the exact same model, including its weights and the optimizer\n",
        "new_model = tf.keras.models.load_model('my_model.h5')\n",
        "\n",
        "# Show the model architecture\n",
        "new_model.summary()\n",
        "\n",
        "loss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n",
        "\n",
        "\n",
        "# SavedModel format\n",
        "# Create and train a new model instance.\n",
        "model = create_model()\n",
        "model.fit(train_images, train_labels, epochs=5)\n",
        "\n",
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/my_model') \n",
        "\n",
        "# my_model directory\n",
        "!ls saved_model\n",
        "\n",
        "# Contains an assets folder, saved_model.pb, and variables folder.\n",
        "!ls saved_model/my_model\n",
        "\n",
        "new_model = tf.keras.models.load_model('saved_model/my_model')\n",
        "\n",
        "# Check its architecture\n",
        "new_model.summary()\n",
        "\n",
        "# Evaluate the restored model\n",
        "loss, acc = new_model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n",
        "\n",
        "print(new_model.predict(test_images).shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}